{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824adc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: h:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\notebooks\n",
      "Repo root: h:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\notebooks\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "nb_dir = Path.cwd()\n",
    "\n",
    "# 你在 repo 根目录运行 -> repo_root = nb_dir\n",
    "# 你在 notebooks 目录运行 -> repo_root = nb_dir.parent\n",
    "repo_root = nb_dir\n",
    "# repo_root = nb_dir.parent\n",
    "\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(\"CWD:\", nb_dir)\n",
    "print(\"Repo root:\", repo_root)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a22761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_dir: H:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\data\\processed\n",
      "constraints_npz: H:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\data\\processed\\constraints.npz\n",
      "Dataset size: 300\n",
      "AI mean/std: 7.208985805511475 1.2618153095245361\n",
      "Seis mean/std: 7.208985805511475 1.2618153095245361\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from src.geo_constraints import DataPaths\n",
    "from src.dataset_vie import StanfordVIEWellPatchDataset\n",
    "\n",
    "DATA_ROOT = r\"H:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\data\"\n",
    "paths = DataPaths(DATA_ROOT)\n",
    "\n",
    "constraints_npz = os.path.join(paths.processed_dir, \"constraints.npz\")\n",
    "assert os.path.isfile(constraints_npz), f\"Missing constraints_npz: {constraints_npz}\"\n",
    "\n",
    "ds = StanfordVIEWellPatchDataset(\n",
    "    paths,\n",
    "    constraints_npz,\n",
    "    patch_hw=4,\n",
    "    use_masked_y=True,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "print(\"processed_dir:\", paths.processed_dir)\n",
    "print(\"constraints_npz:\", constraints_npz)\n",
    "print(\"Dataset size:\", len(ds))\n",
    "print(\"AI mean/std:\", ds.ai_mean, ds.ai_std)\n",
    "print(\"Seis mean/std:\", ds.seis_mean, ds.seis_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45333b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded splits: H:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\data\\processed\\splits\n",
      "train: 210 val: 45 test: 45\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "split_dir = os.path.join(paths.processed_dir, \"splits\")\n",
    "os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "train_f = os.path.join(split_dir, \"train_idx.npy\")\n",
    "val_f   = os.path.join(split_dir, \"val_idx.npy\")\n",
    "test_f  = os.path.join(split_dir, \"test_idx.npy\")\n",
    "\n",
    "def make_splits(n, seed=2026, frac_train=0.8, frac_val=0.1):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n, dtype=np.int32)\n",
    "    rng.shuffle(idx)\n",
    "    n_train = int(frac_train * n)\n",
    "    n_val   = int(frac_val   * n)\n",
    "    train = idx[:n_train]\n",
    "    val   = idx[n_train:n_train+n_val]\n",
    "    test  = idx[n_train+n_val:]\n",
    "    return train, val, test\n",
    "\n",
    "if os.path.isfile(train_f) and os.path.isfile(val_f) and os.path.isfile(test_f):\n",
    "    train_idx = np.load(train_f)\n",
    "    val_idx   = np.load(val_f)\n",
    "    test_idx  = np.load(test_f)\n",
    "    print(\"Loaded splits:\", split_dir)\n",
    "else:\n",
    "    train_idx, val_idx, test_idx = make_splits(len(ds), seed=2026)\n",
    "    np.save(train_f, train_idx)\n",
    "    np.save(val_f, val_idx)\n",
    "    np.save(test_f, test_idx)\n",
    "    print(\"Created splits:\", split_dir)\n",
    "\n",
    "print(\"train:\", len(train_idx), \"val:\", len(val_idx), \"test:\", len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9dfba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_joint: H:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\data\\processed\\checkpoints_multitask_final\\best_joint.pt\n",
      "ckpt_ai   : H:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\data\\processed\\checkpoints_multitask_final\\best_ai.pt\n",
      "Loaded best_joint & best_ai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_38724\\2082691218.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from src.models.geo_cnn_multitask import GeoCNNMultiTask\n",
    "\n",
    "ckpt_dir_final = os.path.join(paths.processed_dir, \"checkpoints_multitask_final\")\n",
    "ckpt_joint = os.path.join(ckpt_dir_final, \"best_joint.pt\")\n",
    "ckpt_ai    = os.path.join(ckpt_dir_final, \"best_ai.pt\")\n",
    "\n",
    "assert os.path.isfile(ckpt_joint), f\"Missing: {ckpt_joint}\"\n",
    "assert os.path.isfile(ckpt_ai),    f\"Missing: {ckpt_ai}\"\n",
    "\n",
    "print(\"ckpt_joint:\", ckpt_joint)\n",
    "print(\"ckpt_ai   :\", ckpt_ai)\n",
    "\n",
    "def build_model():\n",
    "    # MUST match training\n",
    "    return GeoCNNMultiTask(\n",
    "        in_channels=7,\n",
    "        base=32,\n",
    "        t=200,\n",
    "        n_facies=4\n",
    "    ).to(device)\n",
    "\n",
    "def load_ckpt(model, ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    state = ckpt[\"model_state\"] if isinstance(ckpt, dict) and \"model_state\" in ckpt else ckpt\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "    return ckpt\n",
    "\n",
    "m_joint = build_model()\n",
    "m_ai    = build_model()\n",
    "_ = load_ckpt(m_joint, ckpt_joint)\n",
    "_ = load_ckpt(m_ai, ckpt_ai)\n",
    "\n",
    "print(\"Loaded best_joint & best_ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994f3fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physics forward: OK\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "try:\n",
    "    from src.physics_forward import forward_seismic_from_ai\n",
    "    HAS_PHYS = True\n",
    "    print(\"Physics forward: OK\")\n",
    "except Exception as e:\n",
    "    HAS_PHYS = False\n",
    "    print(\"Physics forward: NOT available -> will skip L2/NRMS. Error:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe456ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def unpack_outputs(out):\n",
    "    \"\"\"\n",
    "    returns: ai_pred, facies_logits (optional)\n",
    "    \"\"\"\n",
    "    ai_pred = None\n",
    "    facies_logits = None\n",
    "    if isinstance(out, dict):\n",
    "        for k in [\"ai\", \"ai_pred\", \"y\", \"imp\", \"impedance\"]:\n",
    "            if k in out:\n",
    "                ai_pred = out[k]; break\n",
    "        for k in [\"facies\", \"facies_logits\", \"logits\", \"facies_logit\"]:\n",
    "            if k in out:\n",
    "                facies_logits = out[k]; break\n",
    "    elif isinstance(out, (list, tuple)):\n",
    "        if len(out) >= 1: ai_pred = out[0]\n",
    "        if len(out) >= 2: facies_logits = out[1]\n",
    "    else:\n",
    "        ai_pred = out\n",
    "    return ai_pred, facies_logits\n",
    "\n",
    "def mask_center_trace(m_patch):\n",
    "    \"\"\"\n",
    "    m_patch: [1,H,W,T] or [B,1,H,W,T]\n",
    "    \"\"\"\n",
    "    if m_patch.ndim == 4:\n",
    "        _, H, W, T = m_patch.shape\n",
    "        return m_patch[0, H//2, W//2, :]\n",
    "    elif m_patch.ndim == 5:\n",
    "        B, _, H, W, T = m_patch.shape\n",
    "        return m_patch[:, 0, H//2, W//2, :]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected m_patch shape: {m_patch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e40dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "EPS = 1e-12\n",
    "\n",
    "def _flat_apply_mask(y_pred, y_true, mask=None):\n",
    "    yp = np.asarray(y_pred, dtype=np.float32).reshape(-1)\n",
    "    yt = np.asarray(y_true, dtype=np.float32).reshape(-1)\n",
    "    if mask is None:\n",
    "        m = np.ones_like(yt, dtype=bool)\n",
    "    else:\n",
    "        m = np.asarray(mask, dtype=bool).reshape(-1)\n",
    "    return yp[m], yt[m]\n",
    "\n",
    "def MAE(y_pred, y_true, mask=None):\n",
    "    yp, yt = _flat_apply_mask(y_pred, y_true, mask)\n",
    "    return float(np.mean(np.abs(yp - yt)) if yp.size else np.nan)\n",
    "\n",
    "def RMSE(y_pred, y_true, mask=None):\n",
    "    yp, yt = _flat_apply_mask(y_pred, y_true, mask)\n",
    "    return float(np.sqrt(np.mean((yp - yt) ** 2)) if yp.size else np.nan)\n",
    "\n",
    "def RE(y_pred, y_true, mask=None, eps=EPS):\n",
    "    yp, yt = _flat_apply_mask(y_pred, y_true, mask)\n",
    "    return float(np.mean(np.abs(yp - yt) / (np.abs(yt) + eps)) if yp.size else np.nan)\n",
    "\n",
    "def PearsonR(y_pred, y_true, mask=None, eps=EPS):\n",
    "    yp, yt = _flat_apply_mask(y_pred, y_true, mask)\n",
    "    if yp.size < 2:\n",
    "        return float(\"nan\")\n",
    "    yp0 = yp - yp.mean()\n",
    "    yt0 = yt - yt.mean()\n",
    "    denom = (np.sqrt(np.sum(yp0**2)) * np.sqrt(np.sum(yt0**2)) + eps)\n",
    "    return float(np.sum(yp0 * yt0) / denom)\n",
    "\n",
    "def SSIM_global_2d(x, y, beta1=1e-4, beta2=9e-4, mask=None):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    assert x.shape == y.shape, f\"SSIM shape mismatch: {x.shape} vs {y.shape}\"\n",
    "\n",
    "    if mask is not None:\n",
    "        m = np.asarray(mask, dtype=bool)\n",
    "        xv = x[m]; yv = y[m]\n",
    "    else:\n",
    "        xv = x.reshape(-1); yv = y.reshape(-1)\n",
    "\n",
    "    if xv.size < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    mu_x = float(np.mean(xv))\n",
    "    mu_y = float(np.mean(yv))\n",
    "    sig_x = float(np.std(xv, ddof=0))\n",
    "    sig_y = float(np.std(yv, ddof=0))\n",
    "    sig_xy = float(np.mean((xv - mu_x) * (yv - mu_y)))\n",
    "\n",
    "    num = (2 * mu_x * mu_y + beta1) * (2 * sig_xy + beta2)\n",
    "    den = (mu_x**2 + mu_y**2 + beta1) * (sig_x**2 + sig_y**2 + beta2)\n",
    "    return float(num / (den + EPS))\n",
    "\n",
    "def L2_misfit_norm(d_syn, d_obs, mask=None, eps=EPS):\n",
    "    dsyn = np.asarray(d_syn, dtype=np.float32).reshape(-1)\n",
    "    dobs = np.asarray(d_obs, dtype=np.float32).reshape(-1)\n",
    "    if mask is not None:\n",
    "        m = np.asarray(mask, dtype=bool).reshape(-1)\n",
    "        dsyn = dsyn[m]; dobs = dobs[m]\n",
    "    if dsyn.size == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.linalg.norm(dsyn - dobs) / (np.linalg.norm(dobs) + eps))\n",
    "\n",
    "def RMS(x):\n",
    "    x = np.asarray(x, dtype=np.float32).reshape(-1)\n",
    "    return float(np.sqrt(np.mean(x**2)) if x.size else np.nan)\n",
    "\n",
    "def NRMS(d_syn, d_obs, mask=None, eps=EPS):\n",
    "    dsyn = np.asarray(d_syn, dtype=np.float32)\n",
    "    dobs = np.asarray(d_obs, dtype=np.float32)\n",
    "    if mask is not None:\n",
    "        m = np.asarray(mask, dtype=bool)\n",
    "        dsyn = dsyn[m]; dobs = dobs[m]\n",
    "    if dsyn.size == 0:\n",
    "        return float(\"nan\")\n",
    "    num = 2.0 * RMS(dsyn - dobs)\n",
    "    den = (RMS(dsyn) + RMS(dobs) + eps)\n",
    "    return float((num / den) * 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2737d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "@torch.no_grad()\n",
    "def collect_center_traces(model, indices, name=\"model\"):\n",
    "    ai_true_den = []\n",
    "    ai_pred_den = []\n",
    "    mask_all    = []\n",
    "    seis_obs_den_all = []\n",
    "    seis_syn_den_all = [] if HAS_PHYS else None\n",
    "\n",
    "    il_list, xl_list, wn_list = [], [], []\n",
    "\n",
    "    for idx in indices.tolist():\n",
    "        b = ds[int(idx)]\n",
    "\n",
    "        x = b[\"x\"][None].to(device)  # [1,1,H,W,T]\n",
    "        p = b[\"p\"][None].to(device)\n",
    "        c = b[\"c\"][None].to(device)\n",
    "        m = b[\"m\"][None].to(device)\n",
    "\n",
    "        out = model(x, p, c, m)\n",
    "        ai_pred, _ = unpack_outputs(out)\n",
    "\n",
    "        ai_pred = ai_pred.squeeze()\n",
    "        if ai_pred.ndim != 1:\n",
    "            ai_pred = ai_pred.reshape(-1)  # [T]\n",
    "\n",
    "        y = b[\"y\"].float()  # [T] norm\n",
    "        mc = mask_center_trace(b[\"m\"]).float()\n",
    "        mask = (mc.numpy() > 0.5)  # bool [T]\n",
    "\n",
    "        # denorm AI\n",
    "        y_den = (y.numpy() * float(ds.ai_std) + float(ds.ai_mean)).astype(np.float32)\n",
    "        p_den = (ai_pred.detach().cpu().numpy() * float(ds.ai_std) + float(ds.ai_mean)).astype(np.float32)\n",
    "\n",
    "        # observed seismic center trace denorm\n",
    "        x_patch = b[\"x\"].float()  # [1,H,W,T] norm\n",
    "        H = x_patch.shape[1]; W = x_patch.shape[2]\n",
    "        seis_obs_norm = x_patch[0, H//2, W//2, :].numpy().astype(np.float32)\n",
    "        seis_obs_den  = seis_obs_norm * float(ds.seis_std) + float(ds.seis_mean)\n",
    "\n",
    "        ai_true_den.append(y_den)\n",
    "        ai_pred_den.append(p_den)\n",
    "        mask_all.append(mask)\n",
    "        seis_obs_den_all.append(seis_obs_den)\n",
    "\n",
    "        # forward seismic\n",
    "        if HAS_PHYS:\n",
    "            ai_den_t = torch.from_numpy(p_den).to(device).float()[None, :]  # [1,T]\n",
    "            try:\n",
    "                seis_syn = forward_seismic_from_ai(ai_den_t)\n",
    "                seis_syn = seis_syn.squeeze().detach().cpu().numpy().astype(np.float32).reshape(-1)\n",
    "            except Exception:\n",
    "                seis_syn = np.full_like(seis_obs_den, np.nan, dtype=np.float32)\n",
    "            seis_syn_den_all.append(seis_syn)\n",
    "\n",
    "        il_list.append(int(b[\"il\"]))\n",
    "        xl_list.append(int(b[\"xl\"]))\n",
    "        wn_list.append(str(b[\"wellname\"]))\n",
    "\n",
    "    pack = {\n",
    "        \"name\": name,\n",
    "        \"ai_true_den\": np.stack(ai_true_den, axis=0),     # [N,T]\n",
    "        \"ai_pred_den\": np.stack(ai_pred_den, axis=0),     # [N,T]\n",
    "        \"mask\":        np.stack(mask_all, axis=0),        # [N,T]\n",
    "        \"seis_obs_den\":np.stack(seis_obs_den_all, axis=0),# [N,T]\n",
    "        \"il\": il_list,\n",
    "        \"xl\": xl_list,\n",
    "        \"wellname\": wn_list\n",
    "    }\n",
    "    pack[\"seis_syn_den\"] = np.stack(seis_syn_den_all, axis=0) if HAS_PHYS else None\n",
    "    return pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47289ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def compute_3p1p2_metrics(pack):\n",
    "    y = pack[\"ai_true_den\"]  # [N,T]\n",
    "    p = pack[\"ai_pred_den\"]  # [N,T]\n",
    "    m = pack[\"mask\"].astype(bool)\n",
    "\n",
    "    N, T = y.shape\n",
    "    met = {}\n",
    "    met[\"n_traces\"] = int(N)\n",
    "    met[\"n_samples_masked\"] = int(np.sum(m))\n",
    "\n",
    "    # impedance: masked\n",
    "    met[\"AI_MAE_masked\"]  = MAE(p, y, m)\n",
    "    met[\"AI_RMSE_masked\"] = RMSE(p, y, m)\n",
    "    met[\"AI_RE_masked\"]   = RE(p, y, m)\n",
    "    met[\"AI_R_masked\"]    = PearsonR(p, y, m)\n",
    "\n",
    "    # impedance: all\n",
    "    met[\"AI_MAE_all\"]  = MAE(p, y, None)\n",
    "    met[\"AI_RMSE_all\"] = RMSE(p, y, None)\n",
    "    met[\"AI_RE_all\"]   = RE(p, y, None)\n",
    "    met[\"AI_R_all\"]    = PearsonR(p, y, None)\n",
    "\n",
    "    # SSIM on stacked 2D section matrix [T,N]\n",
    "    order = np.lexsort((np.asarray(pack[\"xl\"]), np.asarray(pack[\"il\"])))\n",
    "    y_sec = y[order].T\n",
    "    p_sec = p[order].T\n",
    "    m_sec = m[order].T\n",
    "    met[\"AI_SSIM_section_masked\"] = SSIM_global_2d(p_sec, y_sec, beta1=1e-4, beta2=9e-4, mask=m_sec)\n",
    "    met[\"AI_SSIM_section_all\"]    = SSIM_global_2d(p_sec, y_sec, beta1=1e-4, beta2=9e-4, mask=None)\n",
    "\n",
    "    # seismic consistency\n",
    "    if pack[\"seis_syn_den\"] is not None:\n",
    "        d_obs = pack[\"seis_obs_den\"]\n",
    "        d_syn = pack[\"seis_syn_den\"]\n",
    "        met[\"Seis_L2_masked\"]      = L2_misfit_norm(d_syn, d_obs, m)\n",
    "        met[\"Seis_NRMS_masked(%)\"] = NRMS(d_syn, d_obs, m)\n",
    "        met[\"Seis_L2_all\"]         = L2_misfit_norm(d_syn, d_obs, None)\n",
    "        met[\"Seis_NRMS_all(%)\"]    = NRMS(d_syn, d_obs, None)\n",
    "    else:\n",
    "        met[\"Seis_L2_masked\"]      = None\n",
    "        met[\"Seis_NRMS_masked(%)\"] = None\n",
    "        met[\"Seis_L2_all\"]         = None\n",
    "        met[\"Seis_NRMS_all(%)\"]    = None\n",
    "\n",
    "    return met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "886d81c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting test predictions...\n",
      "Computing 3.1.2 metrics...\n",
      "Saved JSON: H:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\data\\processed\\eval_reports\\metrics_3p1p2_test.json\n",
      "Saved CSV : H:\\GK-MRL-PhysicsConsistent-Inversion\\GK-MRL-PhysicsConsistent-Inversion\\data\\processed\\eval_reports\\metrics_3p1p2_test.csv\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "OUT_DIR = os.path.join(paths.processed_dir, \"eval_reports\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_JSON = os.path.join(OUT_DIR, \"metrics_3p1p2_test.json\")\n",
    "OUT_CSV  = os.path.join(OUT_DIR, \"metrics_3p1p2_test.csv\")\n",
    "\n",
    "print(\"Collecting test predictions...\")\n",
    "pack_ai    = collect_center_traces(m_ai,    test_idx, name=\"best_ai\")\n",
    "pack_joint = collect_center_traces(m_joint, test_idx, name=\"best_joint\")\n",
    "\n",
    "print(\"Computing 3.1.2 metrics...\")\n",
    "met_ai    = compute_3p1p2_metrics(pack_ai)\n",
    "met_joint = compute_3p1p2_metrics(pack_joint)\n",
    "\n",
    "report = {\n",
    "    \"data_root\": DATA_ROOT,\n",
    "    \"processed_dir\": paths.processed_dir,\n",
    "    \"constraints_npz\": constraints_npz,\n",
    "    \"ckpt_dir\": ckpt_dir_final,\n",
    "    \"ckpt_best_ai\": ckpt_ai,\n",
    "    \"ckpt_best_joint\": ckpt_joint,\n",
    "    \"physics_forward_available\": bool(HAS_PHYS),\n",
    "    \"metrics_3p1p2\": {\n",
    "        \"best_ai\": met_ai,\n",
    "        \"best_joint\": met_joint\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "print(\"Saved JSON:\", OUT_JSON)\n",
    "\n",
    "# CSV\n",
    "try:\n",
    "    import pandas as pd\n",
    "    rows = []\n",
    "    for name, met in report[\"metrics_3p1p2\"].items():\n",
    "        row = {\"model\": name}\n",
    "        row.update(met)\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved CSV :\", OUT_CSV)\n",
    "    df\n",
    "except Exception as e:\n",
    "    print(\"CSV save failed:\", repr(e))\n",
    "    print(\"best_ai metrics:\", met_ai)\n",
    "    print(\"best_joint metrics:\", met_joint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_inv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
